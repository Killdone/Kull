{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159566</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159567</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159568</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159569</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159570</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159566  \":::::And for the second time of asking, when ...      0\n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159569  And it looks like it was actually you who put ...      0\n",
       "159570  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 253 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['text'] = df['text'].str.lower() \n",
    "\n",
    "\n",
    "df1 = df.sample(frac=0.1).reset_index(drop=True)\n",
    "\n",
    "#df1 = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> лематизируем данные и очистим их от стопслов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U textblob --user\n",
    "\n",
    "#!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U textblob --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Killdone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#Используем текстблог с постегом для привода предложения в изначальную форму\n",
    "%time\n",
    "def lemmatize_with_postag(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "#sentence = \"The striped bats are hanging on their feet for best\"\n",
    "#lemmatize_with_postag(sentence)\n",
    "#> 'The striped bat be hang on their foot for best'\n",
    "df1['text_lemm'] = df1['text'].apply(lambda x: lemmatize_with_postag(x))\n",
    "#corpus = list(df['text'])\n",
    "#lemmatize_with_postag(corpus[0])\n",
    "#> 'The striped bat be hang on their foot for best'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На 80к данных\n",
    "CPU times: user 8min 56s, sys: 7.54 s, total: 9min 3s\n",
    "Wall time: 9min 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>new sock of the editor of all things wikipedia...</td>\n",
       "      <td>0</td>\n",
       "      <td>new sock of the editor of all thing wikipedia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>deletion discussion about monopoly (star wars)...</td>\n",
       "      <td>0</td>\n",
       "      <td>deletion discussion about monopoly star war he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>~no one has mentioned gambling in montana, so ...</td>\n",
       "      <td>0</td>\n",
       "      <td>~no one have mention gamble in montana so i do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>though i cannot comment on the distribution la...</td>\n",
       "      <td>0</td>\n",
       "      <td>though i can not comment on the distribution l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>, about a street elephant named pang dao</td>\n",
       "      <td>0</td>\n",
       "      <td>about a street elephant name pang dao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15952</td>\n",
       "      <td>\"\\n\\nan added remark in my article that will p...</td>\n",
       "      <td>0</td>\n",
       "      <td>an added remark in my article that will please...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15953</td>\n",
       "      <td>\"::::i fear they might have intellectual prope...</td>\n",
       "      <td>0</td>\n",
       "      <td>:i fear they might have intellectual property ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15954</td>\n",
       "      <td>funny how you constantly make up lies. i have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>funny how you constantly make up lie i have no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15955</td>\n",
       "      <td>beerstraaten or beerstraten?\\nin the dutch lan...</td>\n",
       "      <td>0</td>\n",
       "      <td>beerstraaten or beerstraten in the dutch langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15956</td>\n",
       "      <td>wooooooooooo thats interesting</td>\n",
       "      <td>0</td>\n",
       "      <td>wooooooooooo thats interest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15957 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  toxic  \\\n",
       "0      new sock of the editor of all things wikipedia...      0   \n",
       "1      deletion discussion about monopoly (star wars)...      0   \n",
       "2      ~no one has mentioned gambling in montana, so ...      0   \n",
       "3      though i cannot comment on the distribution la...      0   \n",
       "4               , about a street elephant named pang dao      0   \n",
       "...                                                  ...    ...   \n",
       "15952  \"\\n\\nan added remark in my article that will p...      0   \n",
       "15953  \"::::i fear they might have intellectual prope...      0   \n",
       "15954  funny how you constantly make up lies. i have ...      0   \n",
       "15955  beerstraaten or beerstraten?\\nin the dutch lan...      0   \n",
       "15956                     wooooooooooo thats interesting      0   \n",
       "\n",
       "                                               text_lemm  \n",
       "0      new sock of the editor of all thing wikipedia ...  \n",
       "1      deletion discussion about monopoly star war he...  \n",
       "2      ~no one have mention gamble in montana so i do...  \n",
       "3      though i can not comment on the distribution l...  \n",
       "4                  about a street elephant name pang dao  \n",
       "...                                                  ...  \n",
       "15952  an added remark in my article that will please...  \n",
       "15953  :i fear they might have intellectual property ...  \n",
       "15954  funny how you constantly make up lie i have no...  \n",
       "15955  beerstraaten or beerstraten in the dutch langu...  \n",
       "15956                        wooooooooooo thats interest  \n",
       "\n",
       "[15957 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>new sock of the editor of all things wikipedia...</td>\n",
       "      <td>0</td>\n",
       "      <td>new sock of the editor of all things wikipedia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>deletion discussion about monopoly (star wars)...</td>\n",
       "      <td>0</td>\n",
       "      <td>deletion discussion about monopoly star wars h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>~no one has mentioned gambling in montana, so ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no one has mentioned gambling in montana so i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>though i cannot comment on the distribution la...</td>\n",
       "      <td>0</td>\n",
       "      <td>though i cannot comment on the distribution la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>, about a street elephant named pang dao</td>\n",
       "      <td>0</td>\n",
       "      <td>about a street elephant named pang dao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15952</td>\n",
       "      <td>\"\\n\\nan added remark in my article that will p...</td>\n",
       "      <td>0</td>\n",
       "      <td>an added remark in my article that will please...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15953</td>\n",
       "      <td>\"::::i fear they might have intellectual prope...</td>\n",
       "      <td>0</td>\n",
       "      <td>i fear they might have intellectual property i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15954</td>\n",
       "      <td>funny how you constantly make up lies. i have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>funny how you constantly make up lies i have n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15955</td>\n",
       "      <td>beerstraaten or beerstraten?\\nin the dutch lan...</td>\n",
       "      <td>0</td>\n",
       "      <td>beerstraaten or beerstraten in the dutch langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15956</td>\n",
       "      <td>wooooooooooo thats interesting</td>\n",
       "      <td>0</td>\n",
       "      <td>wooooooooooo thats interesting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15957 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  toxic  \\\n",
       "0      new sock of the editor of all things wikipedia...      0   \n",
       "1      deletion discussion about monopoly (star wars)...      0   \n",
       "2      ~no one has mentioned gambling in montana, so ...      0   \n",
       "3      though i cannot comment on the distribution la...      0   \n",
       "4               , about a street elephant named pang dao      0   \n",
       "...                                                  ...    ...   \n",
       "15952  \"\\n\\nan added remark in my article that will p...      0   \n",
       "15953  \"::::i fear they might have intellectual prope...      0   \n",
       "15954  funny how you constantly make up lies. i have ...      0   \n",
       "15955  beerstraaten or beerstraten?\\nin the dutch lan...      0   \n",
       "15956                     wooooooooooo thats interesting      0   \n",
       "\n",
       "                                               text_lemm  \n",
       "0      new sock of the editor of all things wikipedia...  \n",
       "1      deletion discussion about monopoly star wars h...  \n",
       "2      no one has mentioned gambling in montana so i ...  \n",
       "3      though i cannot comment on the distribution la...  \n",
       "4                 about a street elephant named pang dao  \n",
       "...                                                  ...  \n",
       "15952  an added remark in my article that will please...  \n",
       "15953  i fear they might have intellectual property i...  \n",
       "15954  funny how you constantly make up lies i have n...  \n",
       "15955  beerstraaten or beerstraten in the dutch langu...  \n",
       "15956                     wooooooooooo thats interesting  \n",
       "\n",
       "[15957 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Очистим текст и разделим соеденим слов, разделяя их пробелом\n",
    "df1['text_lemm'] = df1['text'].apply(lambda x: re.sub(r'[^a-zA-Z\\']', ' ', x))\n",
    "df1['text_lemm'] = df1['text_lemm'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "corpus = df1['text_lemm']\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Способ без BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "#from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df1['text_lemm'].values\n",
    "target = df1['toxic']\n",
    " \n",
    "corpus_train, corpus_test, target_train, target_test = train_test_split(corpus, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Killdone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Очистим текст от стопслов и превратим в признаки\n",
    "nltk.download('stopwords')\n",
    " \n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "count_tf_idf.fit(corpus_train)\n",
    " \n",
    "features_train = count_tf_idf.transform(corpus_train)\n",
    "features_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch===1.5.1 torchvision===0.6.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.sample(2000).reset_index(drop=True) #Возьмем небольшой кусок датасета "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       i agree leave the links alone already but insh...\n",
       "1       assistance at cultural appropriation hi i'd ap...\n",
       "2       i guess not since it is stated by neutral hist...\n",
       "3       barnstar the excellent user page award to date...\n",
       "4       complain about your disruptive behavior here h...\n",
       "                              ...                        \n",
       "1995    hi qqq please note that pages in the user name...\n",
       "1996    hi danimations i have some suggestions firstly...\n",
       "1997    decausa your pissing me off first of all i hav...\n",
       "1998    you're always the victim aren't you jzg i'll b...\n",
       "1999    impact on plastics recycling does rubber tough...\n",
       "Name: text_lemm, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['text_lemm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3b0277ecc44170bb2e69cf766513ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#random.seed(400)\n",
    "#data_bert=data.loc[random.sample(range(len(data)),1200)]\n",
    "\n",
    "tokenizer = transformers.BertTokenizer( vocab_file='vocab.txt')\n",
    "tokenized = df2['text_lemm'].apply(lambda x: tokenizer.encode(x[:512], add_special_tokens=True))\n",
    "n = max(map(len, tokenized))\n",
    "\n",
    "config = transformers.BertConfig.from_json_file(\n",
    "    'bert-base-uncased-config.json')\n",
    "model = transformers.BertModel.from_pretrained(\n",
    "    'bert-base-uncased-pytorch_model.bin', config=config)\n",
    "\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(tokenized.shape[0] // batch_size)):\n",
    "    \n",
    "        padded = np.array([i + [0]*(n - len(i)) for i in tokenized.values[batch_size*i:batch_size*(i+1)]])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        \n",
    "        batch = torch.LongTensor(padded) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 768)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.concatenate(embeddings)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_b,features_test_b, target_train_b,target_test_b=train_test_split(features, df2['toxic'], test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7098591549295774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_lr = LogisticRegression(class_weight = 'balanced', solver='liblinear', random_state=12345)\n",
    " \n",
    "model_lr.fit(features_train, target_train) \n",
    "predict = model_lr.predict(features_test)\n",
    "print(f1_score(target_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LogisticRegression BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert f1_score: 0.24999999999999994\n"
     ]
    }
   ],
   "source": [
    "model_lrb = LogisticRegression(class_weight = 'balanced', solver='liblinear', random_state=12345)\n",
    "model_lrb.fit(features_train_b, target_train_b)\n",
    "predictions= model_lrb.predict(features_test_b)\n",
    "print('bert f1_score:', f1_score(target_test_b, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "#from catboost import CatBoostClassifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44187963726298435\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_depth = 15, n_estimators = 40, class_weight = 'balanced', random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "predict = model.predict(features_test)\n",
    "print(f1_score(target_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier()\n",
    "parameters = {'max_depth'         : [10, 15, 30, 50],\n",
    "                  'n_estimators' : [10, 20, 40, 60],\n",
    "                  \n",
    "                 }\n",
    "grid = GridSearchCV(estimator=model, param_grid = parameters, cv = 2, n_jobs=-1, scoring='f1')\n",
    "grid.fit(features_train, target_train)    \n",
    "grid.predict(features_test)\n",
    "         \n",
    "    \n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",\n",
    "              grid.best_estimator_)\n",
    "    \n",
    "print(\"\\n The best score across ALL searched params:\\n\",\n",
    "              grid.best_score_)\n",
    "    \n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",\n",
    "              grid.best_params_)\n",
    "print(\"\\n ========================================================\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(target_test, grid.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>RandomForestClassifier BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_depth = 15, n_estimators = 40, class_weight = 'balanced', random_state=12345)\n",
    "model.fit(features_train_b, target_train_b)\n",
    "predict = model.predict(features_test_b)\n",
    "print(f1_score(target_test_b, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2806386\ttotal: 4.54s\tremaining: 2m 11s\n",
      "1:\tlearn: 0.2408580\ttotal: 8.74s\tremaining: 2m 2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-24d2ede80905>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12345\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Python\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, text_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[0;32m   4113\u001b[0m         self._fit(X, y, cat_features, text_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0;32m   4114\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n\u001b[0m\u001b[0;32m   4116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Python\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[0;32m   1741\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1742\u001b[0m                 \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1743\u001b[1;33m                 \u001b[0mtrain_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"init_model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1744\u001b[0m             )\n\u001b[0;32m   1745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Python\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1230\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1231\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(depth =10, learning_rate=1, iterations =30, random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "predict = model.predict(features_test)\n",
    "print(f1_score(target_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = CatBoostClassifier()\n",
    "parameters = {'depth'         : [6,8,10],\n",
    "                  'learning_rate' : [0.01, 0.05, 0.1],\n",
    "                  'iterations'    : [30, 50, 100]\n",
    "                 }\n",
    "grid = GridSearchCV(estimator=model, param_grid = parameters, cv = 2, n_jobs=-1, scoring='f1')\n",
    "grid.fit(features_train, target_train)    \n",
    "y_pred_c = grid.predict(features_test)\n",
    "   \n",
    "\n",
    "    # Results from Grid Search\n",
    "print(\"\\n========================================================\")\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"========================================================\")    \n",
    "    \n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",\n",
    "          grid.best_estimator_)\n",
    "    \n",
    "print(\"\\n The best score across ALL searched params:\\n\",\n",
    "          grid.best_score_)\n",
    "    \n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",\n",
    "          grid.best_params_)\n",
    "print(\"\\n ========================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CatBoostClassifier BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2848270\ttotal: 2.36s\tremaining: 21.3s\n",
      "1:\tlearn: 0.2037210\ttotal: 4.7s\tremaining: 18.8s\n",
      "2:\tlearn: 0.1440010\ttotal: 7.1s\tremaining: 16.6s\n",
      "3:\tlearn: 0.1087328\ttotal: 9.43s\tremaining: 14.1s\n",
      "4:\tlearn: 0.0612010\ttotal: 12s\tremaining: 12s\n",
      "5:\tlearn: 0.0471180\ttotal: 14.4s\tremaining: 9.58s\n",
      "6:\tlearn: 0.0385294\ttotal: 16.7s\tremaining: 7.16s\n",
      "7:\tlearn: 0.0352107\ttotal: 19s\tremaining: 4.76s\n",
      "8:\tlearn: 0.0328804\ttotal: 21.4s\tremaining: 2.38s\n",
      "9:\tlearn: 0.0263911\ttotal: 23.8s\tremaining: 0us\n",
      "0.14814814814814817\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(depth =10, learning_rate=1, iterations =10, random_state=12345)\n",
    "model.fit(features_train_b, target_train_b)\n",
    "predict = model.predict(features_test_b)\n",
    "print(f1_score(target_test_b, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По хорошему Линейная Регрессия выдает лучший результат. Она быстро обучается и не тербует настройки параметров. Вероятно ей проще всего работать с текстами(из известных мне моделей). Кэт буст так же справляется с задачей, но крайне долго. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
